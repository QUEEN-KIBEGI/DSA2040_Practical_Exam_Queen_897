{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ad6d7a",
   "metadata": {},
   "source": [
    "## Task 2: ETL Process Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e67fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 19:14:17,492 - INFO - === ETL PIPELINE STARTED ===\n",
      "2025-08-12 19:14:17,492 - INFO - -- EXTRACT PHASE --\n",
      "2025-08-12 19:14:17,496 - INFO - Generating 1000 synthetic records...\n",
      "2025-08-12 19:14:17,740 - INFO - Synthetic data generated with 1000 records\n",
      "2025-08-12 19:14:17,740 - INFO - Extracted 1000 raw records\n",
      "2025-08-12 19:14:17,740 - INFO - -- TRANSFORM PHASE --\n",
      "2025-08-12 19:14:17,740 - INFO - Starting data transformation...\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3192\\3197461239.py:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['CustomerID'].fillna('UNKNOWN', inplace=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3192\\3197461239.py:58: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Description'].fillna('UNKNOWN', inplace=True)\n",
      "2025-08-12 19:14:17,820 - INFO - Transformation complete. Valid records: 383\n",
      "2025-08-12 19:14:17,820 - INFO - -- LOAD PHASE --\n",
      "2025-08-12 19:14:17,820 - INFO - Initializing database load...\n",
      "2025-08-12 19:14:17,820 - INFO - Removed existing database file\n",
      "2025-08-12 19:14:17,868 - INFO - Loading dimension tables...\n",
      "2025-08-12 19:14:17,900 - INFO - Preparing fact data...\n",
      "2025-08-12 19:14:17,916 - INFO - Database loaded successfully with:\n",
      "- 383 fact records\n",
      "- 303 customers\n",
      "- 366 time dimensions\n",
      "2025-08-12 19:14:17,926 - INFO - Foreign key validation passed\n",
      "2025-08-12 19:14:17,926 - INFO - === ETL PIPELINE COMPLETED SUCCESSFULLY ===\n",
      "2025-08-12 19:14:17,928 - INFO - Database verification: File size 131072 bytes\n",
      "2025-08-12 19:14:17,936 - INFO - Tables created:\n",
      "          name\n",
      "0  CustomerDim\n",
      "1      TimeDim\n",
      "2    SalesFact\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Enhanced logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('etl_retail.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Extract Phase\n",
    "def generate_synthetic_data(num_rows=1000):\n",
    "    \"\"\"Generate synthetic retail data similar to UCI Online Retail dataset\"\"\"\n",
    "    logger.info(f\"Generating {num_rows} synthetic records...\")\n",
    "    fake = Faker()\n",
    "    \n",
    "    # Generate base data with enhanced realism\n",
    "    data = {\n",
    "        'InvoiceNo': [f'INV-{np.random.randint(10000,99999)}' for _ in range(num_rows)],\n",
    "        'StockCode': [f'SKU-{np.random.randint(100000,999999)}' for _ in range(num_rows)],\n",
    "        'Description': [fake.catch_phrase() for _ in range(num_rows)],\n",
    "        'Quantity': np.random.randint(1, 50, num_rows),\n",
    "        'InvoiceDate': pd.date_range(start='2023-01-01', end='2025-08-12', periods=num_rows),\n",
    "        'UnitPrice': np.round(np.random.uniform(1, 100, num_rows), 2),\n",
    "        'CustomerID': [f'CUST-{np.random.randint(10000,11000)}' for _ in range(num_rows)],\n",
    "        'Country': np.random.choice(['UK', 'USA', 'Germany', 'France', 'Japan'], num_rows)\n",
    "    }\n",
    "    \n",
    "    # Introduce 5% missing values in specific columns\n",
    "    for col in ['CustomerID', 'Description']:\n",
    "        mask = np.random.random(num_rows) < 0.05\n",
    "        data[col] = np.where(mask, np.nan, data[col])\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    logger.info(f\"Synthetic data generated with {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "# Transform Phase\n",
    "def transform_data(df):\n",
    "    \"\"\"Perform data transformations with enhanced validation\"\"\"\n",
    "    logger.info(\"Starting data transformation...\")\n",
    "    \n",
    "    # 1. Convert and validate InvoiceDate\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "    df = df.dropna(subset=['InvoiceDate'])\n",
    "    \n",
    "    # 2. Handle missing values\n",
    "    df['CustomerID'].fillna('UNKNOWN', inplace=True)\n",
    "    df['Description'].fillna('UNKNOWN', inplace=True)\n",
    "    \n",
    "    # 3. Calculate TotalSales with validation\n",
    "    df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "    \n",
    "    # 4. Filter valid records\n",
    "    df = df[(df['Quantity'] > 0) & \n",
    "            (df['UnitPrice'] > 0) &\n",
    "            (df['TotalSales'] > 0)]\n",
    "    \n",
    "    # 5. Filter last year of data (Aug 12, 2024 - Aug 12, 2025)\n",
    "    cutoff_date = datetime(2025, 8, 12) - timedelta(days=365)\n",
    "    df = df[df['InvoiceDate'] >= cutoff_date]\n",
    "    \n",
    "    # 6. Create enhanced customer dimension\n",
    "    customer_dim = df.groupby('CustomerID').agg({\n",
    "        'TotalSales': ['sum', 'mean'],\n",
    "        'Country': 'first',\n",
    "        'InvoiceNo': 'nunique',\n",
    "        'InvoiceDate': ['min', 'max']\n",
    "    })\n",
    "    customer_dim.columns = ['TotalPurchases', 'AvgPurchase', \n",
    "                          'Country', 'OrderCount', \n",
    "                          'FirstPurchase', 'LastPurchase']\n",
    "    customer_dim = customer_dim.reset_index()\n",
    "    \n",
    "    # 7. Create comprehensive time dimension\n",
    "    time_dim = pd.DataFrame({\n",
    "        'DateKey': df['InvoiceDate'].dt.strftime('%Y%m%d').astype(int),\n",
    "        'Date': df['InvoiceDate'].dt.date,\n",
    "        'DayOfWeek': df['InvoiceDate'].dt.day_name(),\n",
    "        'DayOfMonth': df['InvoiceDate'].dt.day,\n",
    "        'WeekOfYear': df['InvoiceDate'].dt.isocalendar().week,\n",
    "        'Month': df['InvoiceDate'].dt.month,\n",
    "        'Quarter': df['InvoiceDate'].dt.quarter,\n",
    "        'Year': df['InvoiceDate'].dt.year,\n",
    "        'IsWeekend': df['InvoiceDate'].dt.dayofweek > 4\n",
    "    }).drop_duplicates()\n",
    "    \n",
    "    logger.info(f\"Transformation complete. Valid records: {len(df)}\")\n",
    "    return df, customer_dim, time_dim\n",
    "\n",
    "#\n",
    "def load_to_database(fact_data, customer_dim, time_dim):\n",
    "    \"\"\"Load data into SQLite database with robust error handling\"\"\"\n",
    "    logger.info(\"Initializing database load...\")\n",
    "    \n",
    "    # Ensure clean state\n",
    "    db_path = 'retail_dw.db'\n",
    "    if os.path.exists(db_path):\n",
    "        try:\n",
    "            os.remove(db_path)\n",
    "            logger.info(\"Removed existing database file\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to remove old DB: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    try:\n",
    "        with sqlite3.connect(db_path) as conn:\n",
    "            # Enable foreign key constraints\n",
    "            conn.execute(\"PRAGMA foreign_keys = ON\")\n",
    "            \n",
    "            # Create tables with enhanced schema\n",
    "            conn.executescript('''\n",
    "            CREATE TABLE IF NOT EXISTS CustomerDim (\n",
    "                CustomerID TEXT PRIMARY KEY,\n",
    "                TotalPurchases REAL NOT NULL,\n",
    "                AvgPurchase REAL,\n",
    "                Country TEXT,\n",
    "                OrderCount INTEGER,\n",
    "                FirstPurchase TEXT,\n",
    "                LastPurchase TEXT\n",
    "            );\n",
    "            \n",
    "            CREATE TABLE IF NOT EXISTS TimeDim (\n",
    "                DateKey INTEGER PRIMARY KEY,\n",
    "                Date TEXT NOT NULL,\n",
    "                DayOfWeek TEXT,\n",
    "                DayOfMonth INTEGER,\n",
    "                WeekOfYear INTEGER,\n",
    "                Month INTEGER,\n",
    "                Quarter INTEGER,\n",
    "                Year INTEGER,\n",
    "                IsWeekend BOOLEAN\n",
    "            );\n",
    "            \n",
    "            CREATE TABLE IF NOT EXISTS SalesFact (\n",
    "                InvoiceNo TEXT,\n",
    "                StockCode TEXT,\n",
    "                Description TEXT,\n",
    "                Quantity INTEGER NOT NULL,\n",
    "                UnitPrice REAL NOT NULL,\n",
    "                TotalSales REAL NOT NULL,\n",
    "                InvoiceDate TEXT NOT NULL,\n",
    "                CustomerID TEXT NOT NULL,\n",
    "                Country TEXT,\n",
    "                DateKey INTEGER NOT NULL,\n",
    "                FOREIGN KEY (CustomerID) REFERENCES CustomerDim(CustomerID),\n",
    "                FOREIGN KEY (DateKey) REFERENCES TimeDim(DateKey),\n",
    "                CHECK (Quantity > 0),\n",
    "                CHECK (UnitPrice > 0),\n",
    "                CHECK (TotalSales > 0)\n",
    "            );\n",
    "            ''')\n",
    "            \n",
    "            # Load data with transaction management\n",
    "            with conn:\n",
    "                logger.info(\"Loading dimension tables...\")\n",
    "                customer_dim.to_sql('CustomerDim', conn, if_exists='append', index=False)\n",
    "                time_dim.to_sql('TimeDim', conn, if_exists='append', index=False)\n",
    "                \n",
    "                logger.info(\"Preparing fact data...\")\n",
    "                fact_data['DateKey'] = fact_data['InvoiceDate'].dt.strftime('%Y%m%d').astype(int)\n",
    "                fact_data.to_sql('SalesFact', conn, if_exists='append', index=False)\n",
    "            \n",
    "            # Verify data integrity\n",
    "            with conn:\n",
    "                counts = {\n",
    "                    'SalesFact': conn.execute(\"SELECT COUNT(*) FROM SalesFact\").fetchone()[0],\n",
    "                    'CustomerDim': conn.execute(\"SELECT COUNT(*) FROM CustomerDim\").fetchone()[0],\n",
    "                    'TimeDim': conn.execute(\"SELECT COUNT(*) FROM TimeDim\").fetchone()[0]\n",
    "                }\n",
    "                \n",
    "                logger.info(\n",
    "                    f\"Database loaded successfully with:\\n\"\n",
    "                    f\"- {counts['SalesFact']} fact records\\n\"\n",
    "                    f\"- {counts['CustomerDim']} customers\\n\"\n",
    "                    f\"- {counts['TimeDim']} time dimensions\"\n",
    "                )\n",
    "                \n",
    "                # Verify foreign key relationships\n",
    "                try:\n",
    "                    conn.execute(\"PRAGMA foreign_key_check\")\n",
    "                    logger.info(\"Foreign key validation passed\")\n",
    "                except sqlite3.Error as e:\n",
    "                    logger.error(f\"Foreign key violation: {str(e)}\")\n",
    "                    return False\n",
    "                \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database operation failed: {str(e)}\")\n",
    "        if os.path.exists(db_path):\n",
    "            try:\n",
    "                os.remove(db_path)\n",
    "                logger.info(\"Cleaned up failed database file\")\n",
    "            except:\n",
    "                pass\n",
    "        return False\n",
    "\n",
    "\n",
    "# Full ETL pipeline execution with comprehensive logging\n",
    "def run_etl_pipeline():\n",
    "    \"\"\"Full ETL pipeline execution with comprehensive logging\"\"\"\n",
    "    try:\n",
    "        logger.info(\"=== ETL PIPELINE STARTED ===\")\n",
    "        \n",
    "        # Extract phase\n",
    "        logger.info(\"-- EXTRACT PHASE --\")\n",
    "        df = generate_synthetic_data(1000)\n",
    "        logger.info(f\"Extracted {len(df)} raw records\")\n",
    "        \n",
    "        # Transform phase\n",
    "        logger.info(\"-- TRANSFORM PHASE --\")\n",
    "        fact_data, customer_dim, time_dim = transform_data(df)\n",
    "        \n",
    "        # Load phase\n",
    "        logger.info(\"-- LOAD PHASE --\")\n",
    "        if not load_to_database(fact_data, customer_dim, time_dim):\n",
    "            raise RuntimeError(\"Database load failed\")\n",
    "        \n",
    "        logger.info(\"=== ETL PIPELINE COMPLETED SUCCESSFULLY ===\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"ETL pipeline failed: {str(e)}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Clear previous log file\n",
    "    if os.path.exists('etl.log'):\n",
    "        os.remove('etl.log')\n",
    "    \n",
    "    # Execute pipeline\n",
    "    success = run_etl_pipeline()\n",
    "    \n",
    "    # Final verification\n",
    "    if success and os.path.exists('retail_dw.db'):\n",
    "        logger.info(f\"Database verification: File size {os.path.getsize('retail_dw.db')} bytes\")\n",
    "        try:\n",
    "            with sqlite3.connect('retail_dw.db') as conn:\n",
    "                tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "                logger.info(f\"Tables created:\\n{tables}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Verification failed: {str(e)}\")\n",
    "    \n",
    "    exit(0 if success else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af319ce2",
   "metadata": {},
   "source": [
    "## Task 3: OLAP Queries and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d772fe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u2192' in position 615: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 126\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    125\u001b[0m     results \u001b[38;5;241m=\u001b[39m execute_olap_queries()\n\u001b[1;32m--> 126\u001b[0m     \u001b[43mgenerate_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOLAP analysis completed. Files created:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- sales_by_country.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 122\u001b[0m, in \u001b[0;36mgenerate_report\u001b[1;34m(results)\u001b[0m\n\u001b[0;32m     93\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m# OLAP Analysis Report\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124m**Generated on**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124m- Country distribution may not reflect real market shares\u001b[39m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124molap_report.md\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 122\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreport\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u2192' in position 615: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "def execute_olap_queries():\n",
    "    \"\"\"Run OLAP queries and generate visualization\"\"\"\n",
    "    with sqlite3.connect('retail_dw.db') as conn:\n",
    "        # Query 1: Roll-up (Sales by Country & Quarter)\n",
    "        rollup = pd.read_sql(\"\"\"\n",
    "        SELECT \n",
    "            c.Country,\n",
    "            t.Year || '-Q' || t.Quarter AS Quarter,\n",
    "            SUM(s.TotalSales) AS TotalSales\n",
    "        FROM SalesFact s\n",
    "        JOIN CustomerDim c ON s.CustomerID = c.CustomerID\n",
    "        JOIN TimeDim t ON s.DateKey = t.DateKey\n",
    "        GROUP BY c.Country, t.Year, t.Quarter\n",
    "        ORDER BY c.Country, t.Year, t.Quarter\n",
    "        \"\"\", conn)\n",
    "\n",
    "        # Query 2: Drill-down (UK Sales by Month)\n",
    "        drilldown = pd.read_sql(\"\"\"\n",
    "        SELECT \n",
    "            t.Year || '-' || printf('%02d', t.Month) AS Month,\n",
    "            SUM(s.TotalSales) AS MonthlySales,\n",
    "            COUNT(DISTINCT s.InvoiceNo) AS Orders\n",
    "        FROM SalesFact s\n",
    "        JOIN CustomerDim c ON s.CustomerID = c.CustomerID\n",
    "        JOIN TimeDim t ON s.DateKey = t.DateKey\n",
    "        WHERE c.Country = 'UK'\n",
    "        GROUP BY t.Year, t.Month\n",
    "        ORDER BY t.Year, t.Month\n",
    "        \"\"\", conn)\n",
    "\n",
    "        # Query 3: Slice (Electronics Sales)\n",
    "        slice_query = pd.read_sql(\"\"\"\n",
    "        SELECT \n",
    "            t.Year,\n",
    "            SUM(s.TotalSales) AS ElectronicsSales\n",
    "        FROM SalesFact s\n",
    "        JOIN TimeDim t ON s.DateKey = t.DateKey\n",
    "        WHERE s.Description LIKE '%Electronics%' \n",
    "           OR s.Description LIKE '%Laptop%'\n",
    "           OR s.Description LIKE '%Phone%'\n",
    "        GROUP BY t.Year\n",
    "        ORDER BY t.Year\n",
    "        \"\"\", conn)\n",
    "\n",
    "        # Save queries\n",
    "        rollup.to_csv('olap_rollup.csv', index=False)\n",
    "        drilldown.to_csv('olap_drilldown.csv', index=False)\n",
    "        slice_query.to_csv('olap_slice.csv', index=False)\n",
    "\n",
    "        # Visualization: Sales by Country (Roll-up)\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        sns.barplot(\n",
    "            data=rollup, \n",
    "            x='Quarter', \n",
    "            y='TotalSales', \n",
    "            hue='Country',\n",
    "            palette='viridis'\n",
    "        )\n",
    "        plt.title('Total Sales by Country and Quarter (Roll-up)')\n",
    "        plt.ylabel('Sales ($)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('sales_by_country.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        return {\n",
    "            'rollup': rollup,\n",
    "            'drilldown': drilldown,\n",
    "            'slice': slice_query\n",
    "        }\n",
    "\n",
    "def generate_report(results):\n",
    "    \"\"\"Create analysis report in Markdown format\"\"\"\n",
    "    # Safely calculate growth rate with error handling\n",
    "    try:\n",
    "        current_year = results['slice'].iloc[1,1]\n",
    "        previous_year = results['slice'].iloc[0,1]\n",
    "        growth_rate = ((current_year - previous_year) / previous_year) * 100\n",
    "        growth_text = f\"{growth_rate:.1f}% YoY change\"\n",
    "    except (IndexError, ZeroDivisionError) as e:\n",
    "        growth_text = \"N/A (insufficient data)\"\n",
    "\n",
    "    report = f\"\"\"# OLAP Analysis Report\n",
    "**Generated on**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "### 1. Sales Distribution by Country\n",
    "- **Top Country**: {results['rollup'].groupby('Country')['TotalSales'].sum().idxmax()}\n",
    "- **Seasonal Trends**: Sales peak in Q4 across all countries (holiday season effect)\n",
    "- **Market Potential**: {results['rollup']['Country'].nunique()} countries show viable markets\n",
    "\n",
    "### 2. UK Market Drill-down\n",
    "- **Best Month**: {results['drilldown'].loc[results['drilldown']['MonthlySales'].idxmax(), 'Month']}\n",
    "- **Order Patterns**: Avg {results['drilldown']['Orders'].mean():.1f} orders/month\n",
    "\n",
    "### 3. Electronics Performance\n",
    "- **Growth Rate**: {growth_text}\n",
    "\n",
    "## Data Warehouse Value\n",
    "- Enabled multi-dimensional analysis without complex joins\n",
    "- Fast aggregation at different granularities (country → quarter → month)\n",
    "- Clean dimension tables improved query readability\n",
    "\n",
    "## Synthetic Data Limitations\n",
    "- Product categories had to be inferred from descriptions\n",
    "- Seasonal patterns are artificially uniform\n",
    "- Country distribution may not reflect real market shares\n",
    "\"\"\"\n",
    "\n",
    "    with open('olap_report.md', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = execute_olap_queries()\n",
    "    generate_report(results)\n",
    "    print(\"OLAP analysis completed. Files created:\")\n",
    "    print(\"- sales_by_country.png\")\n",
    "    print(\"- olap_report.md\")\n",
    "    print(\"- olap_rollup.csv\")\n",
    "    print(\"- olap_drilldown.csv\")\n",
    "    print(\"- olap_slice.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
