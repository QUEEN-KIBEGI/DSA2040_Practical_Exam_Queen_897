{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ad6d7a",
   "metadata": {},
   "source": [
    "## Task 2: ETL Process Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e67fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 01:43:07,429 - INFO - === ETL PIPELINE STARTED ===\n",
      "2025-08-13 01:43:07,430 - INFO - -- EXTRACT PHASE --\n",
      "2025-08-13 01:43:07,431 - INFO - Generating 1000 synthetic records...\n",
      "2025-08-13 01:43:07,604 - INFO - Synthetic data generated with 1000 records\n",
      "2025-08-13 01:43:07,604 - INFO - Extracted 1000 raw records\n",
      "2025-08-13 01:43:07,604 - INFO - -- TRANSFORM PHASE --\n",
      "2025-08-13 01:43:07,604 - INFO - Starting data transformation...\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_19872\\3197461239.py:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['CustomerID'].fillna('UNKNOWN', inplace=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_19872\\3197461239.py:58: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Description'].fillna('UNKNOWN', inplace=True)\n",
      "2025-08-13 01:43:07,650 - INFO - Transformation complete. Valid records: 383\n",
      "2025-08-13 01:43:07,650 - INFO - -- LOAD PHASE --\n",
      "2025-08-13 01:43:07,650 - INFO - Initializing database load...\n",
      "2025-08-13 01:43:07,663 - INFO - Removed existing database file\n",
      "2025-08-13 01:43:07,695 - INFO - Loading dimension tables...\n",
      "2025-08-13 01:43:07,715 - INFO - Preparing fact data...\n",
      "2025-08-13 01:43:07,734 - INFO - Database loaded successfully with:\n",
      "- 383 fact records\n",
      "- 293 customers\n",
      "- 366 time dimensions\n",
      "2025-08-13 01:43:07,734 - INFO - Foreign key validation passed\n",
      "2025-08-13 01:43:07,734 - INFO - === ETL PIPELINE COMPLETED SUCCESSFULLY ===\n",
      "2025-08-13 01:43:07,734 - INFO - Database verification: File size 126976 bytes\n",
      "2025-08-13 01:43:07,750 - INFO - Tables created:\n",
      "          name\n",
      "0  CustomerDim\n",
      "1      TimeDim\n",
      "2    SalesFact\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Enhanced logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('etl_retail.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Extract Phase\n",
    "def generate_synthetic_data(num_rows=1000):\n",
    "    \"\"\"Generate synthetic retail data similar to UCI Online Retail dataset\"\"\"\n",
    "    logger.info(f\"Generating {num_rows} synthetic records...\")\n",
    "    fake = Faker()\n",
    "    \n",
    "    # Generate base data with enhanced realism\n",
    "    data = {\n",
    "        'InvoiceNo': [f'INV-{np.random.randint(10000,99999)}' for _ in range(num_rows)],\n",
    "        'StockCode': [f'SKU-{np.random.randint(100000,999999)}' for _ in range(num_rows)],\n",
    "        'Description': [fake.catch_phrase() for _ in range(num_rows)],\n",
    "        'Quantity': np.random.randint(1, 50, num_rows),\n",
    "        'InvoiceDate': pd.date_range(start='2023-01-01', end='2025-08-12', periods=num_rows),\n",
    "        'UnitPrice': np.round(np.random.uniform(1, 100, num_rows), 2),\n",
    "        'CustomerID': [f'CUST-{np.random.randint(10000,11000)}' for _ in range(num_rows)],\n",
    "        'Country': np.random.choice(['UK', 'USA', 'Germany', 'France', 'Japan'], num_rows)\n",
    "    }\n",
    "    \n",
    "    # Introduce 5% missing values in specific columns\n",
    "    for col in ['CustomerID', 'Description']:\n",
    "        mask = np.random.random(num_rows) < 0.05\n",
    "        data[col] = np.where(mask, np.nan, data[col])\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    logger.info(f\"Synthetic data generated with {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "# Transform Phase\n",
    "def transform_data(df):\n",
    "    \"\"\"Perform data transformations with enhanced validation\"\"\"\n",
    "    logger.info(\"Starting data transformation...\")\n",
    "    \n",
    "    # 1. Convert and validate InvoiceDate\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "    df = df.dropna(subset=['InvoiceDate'])\n",
    "    \n",
    "    # 2. Handle missing values\n",
    "    df['CustomerID'].fillna('UNKNOWN', inplace=True)\n",
    "    df['Description'].fillna('UNKNOWN', inplace=True)\n",
    "    \n",
    "    # 3. Calculate TotalSales with validation\n",
    "    df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "    \n",
    "    # 4. Filter valid records\n",
    "    df = df[(df['Quantity'] > 0) & \n",
    "            (df['UnitPrice'] > 0) &\n",
    "            (df['TotalSales'] > 0)]\n",
    "    \n",
    "    # 5. Filter last year of data (Aug 12, 2024 - Aug 12, 2025)\n",
    "    cutoff_date = datetime(2025, 8, 12) - timedelta(days=365)\n",
    "    df = df[df['InvoiceDate'] >= cutoff_date]\n",
    "    \n",
    "    # 6. Create enhanced customer dimension\n",
    "    customer_dim = df.groupby('CustomerID').agg({\n",
    "        'TotalSales': ['sum', 'mean'],\n",
    "        'Country': 'first',\n",
    "        'InvoiceNo': 'nunique',\n",
    "        'InvoiceDate': ['min', 'max']\n",
    "    })\n",
    "    customer_dim.columns = ['TotalPurchases', 'AvgPurchase', \n",
    "                          'Country', 'OrderCount', \n",
    "                          'FirstPurchase', 'LastPurchase']\n",
    "    customer_dim = customer_dim.reset_index()\n",
    "    \n",
    "    # 7. Create comprehensive time dimension\n",
    "    time_dim = pd.DataFrame({\n",
    "        'DateKey': df['InvoiceDate'].dt.strftime('%Y%m%d').astype(int),\n",
    "        'Date': df['InvoiceDate'].dt.date,\n",
    "        'DayOfWeek': df['InvoiceDate'].dt.day_name(),\n",
    "        'DayOfMonth': df['InvoiceDate'].dt.day,\n",
    "        'WeekOfYear': df['InvoiceDate'].dt.isocalendar().week,\n",
    "        'Month': df['InvoiceDate'].dt.month,\n",
    "        'Quarter': df['InvoiceDate'].dt.quarter,\n",
    "        'Year': df['InvoiceDate'].dt.year,\n",
    "        'IsWeekend': df['InvoiceDate'].dt.dayofweek > 4\n",
    "    }).drop_duplicates()\n",
    "    \n",
    "    logger.info(f\"Transformation complete. Valid records: {len(df)}\")\n",
    "    return df, customer_dim, time_dim\n",
    "\n",
    "#Load phase\n",
    "def load_to_database(fact_data, customer_dim, time_dim):\n",
    "    \"\"\"Load data into SQLite database with robust error handling\"\"\"\n",
    "    logger.info(\"Initializing database load...\")\n",
    "    \n",
    "    # Ensure clean state\n",
    "    db_path = 'retail_dw.db'\n",
    "    if os.path.exists(db_path):\n",
    "        try:\n",
    "            os.remove(db_path)\n",
    "            logger.info(\"Removed existing database file\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to remove old DB: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    try:\n",
    "        with sqlite3.connect(db_path) as conn:\n",
    "            # Enable foreign key constraints\n",
    "            conn.execute(\"PRAGMA foreign_keys = ON\")\n",
    "            \n",
    "            # Create tables with enhanced schema\n",
    "            conn.executescript('''\n",
    "            CREATE TABLE IF NOT EXISTS CustomerDim (\n",
    "                CustomerID TEXT PRIMARY KEY,\n",
    "                TotalPurchases REAL NOT NULL,\n",
    "                AvgPurchase REAL,\n",
    "                Country TEXT,\n",
    "                OrderCount INTEGER,\n",
    "                FirstPurchase TEXT,\n",
    "                LastPurchase TEXT\n",
    "            );\n",
    "            \n",
    "            CREATE TABLE IF NOT EXISTS TimeDim (\n",
    "                DateKey INTEGER PRIMARY KEY,\n",
    "                Date TEXT NOT NULL,\n",
    "                DayOfWeek TEXT,\n",
    "                DayOfMonth INTEGER,\n",
    "                WeekOfYear INTEGER,\n",
    "                Month INTEGER,\n",
    "                Quarter INTEGER,\n",
    "                Year INTEGER,\n",
    "                IsWeekend BOOLEAN\n",
    "            );\n",
    "            \n",
    "            CREATE TABLE IF NOT EXISTS SalesFact (\n",
    "                InvoiceNo TEXT,\n",
    "                StockCode TEXT,\n",
    "                Description TEXT,\n",
    "                Quantity INTEGER NOT NULL,\n",
    "                UnitPrice REAL NOT NULL,\n",
    "                TotalSales REAL NOT NULL,\n",
    "                InvoiceDate TEXT NOT NULL,\n",
    "                CustomerID TEXT NOT NULL,\n",
    "                Country TEXT,\n",
    "                DateKey INTEGER NOT NULL,\n",
    "                FOREIGN KEY (CustomerID) REFERENCES CustomerDim(CustomerID),\n",
    "                FOREIGN KEY (DateKey) REFERENCES TimeDim(DateKey),\n",
    "                CHECK (Quantity > 0),\n",
    "                CHECK (UnitPrice > 0),\n",
    "                CHECK (TotalSales > 0)\n",
    "            );\n",
    "            ''')\n",
    "            \n",
    "            # Load data with transaction management\n",
    "            with conn:\n",
    "                logger.info(\"Loading dimension tables...\")\n",
    "                customer_dim.to_sql('CustomerDim', conn, if_exists='append', index=False)\n",
    "                time_dim.to_sql('TimeDim', conn, if_exists='append', index=False)\n",
    "                \n",
    "                logger.info(\"Preparing fact data...\")\n",
    "                fact_data['DateKey'] = fact_data['InvoiceDate'].dt.strftime('%Y%m%d').astype(int)\n",
    "                fact_data.to_sql('SalesFact', conn, if_exists='append', index=False)\n",
    "            \n",
    "            # Verify data integrity\n",
    "            with conn:\n",
    "                counts = {\n",
    "                    'SalesFact': conn.execute(\"SELECT COUNT(*) FROM SalesFact\").fetchone()[0],\n",
    "                    'CustomerDim': conn.execute(\"SELECT COUNT(*) FROM CustomerDim\").fetchone()[0],\n",
    "                    'TimeDim': conn.execute(\"SELECT COUNT(*) FROM TimeDim\").fetchone()[0]\n",
    "                }\n",
    "                \n",
    "                logger.info(\n",
    "                    f\"Database loaded successfully with:\\n\"\n",
    "                    f\"- {counts['SalesFact']} fact records\\n\"\n",
    "                    f\"- {counts['CustomerDim']} customers\\n\"\n",
    "                    f\"- {counts['TimeDim']} time dimensions\"\n",
    "                )\n",
    "                \n",
    "                # Verify foreign key relationships\n",
    "                try:\n",
    "                    conn.execute(\"PRAGMA foreign_key_check\")\n",
    "                    logger.info(\"Foreign key validation passed\")\n",
    "                except sqlite3.Error as e:\n",
    "                    logger.error(f\"Foreign key violation: {str(e)}\")\n",
    "                    return False\n",
    "                \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database operation failed: {str(e)}\")\n",
    "        if os.path.exists(db_path):\n",
    "            try:\n",
    "                os.remove(db_path)\n",
    "                logger.info(\"Cleaned up failed database file\")\n",
    "            except:\n",
    "                pass\n",
    "        return False\n",
    "\n",
    "\n",
    "# Full ETL pipeline execution with comprehensive logging\n",
    "def run_etl_pipeline():\n",
    "    \"\"\"Full ETL pipeline execution with comprehensive logging\"\"\"\n",
    "    try:\n",
    "        logger.info(\"=== ETL PIPELINE STARTED ===\")\n",
    "        \n",
    "        # Extract phase\n",
    "        logger.info(\"-- EXTRACT PHASE --\")\n",
    "        df = generate_synthetic_data(1000)\n",
    "        logger.info(f\"Extracted {len(df)} raw records\")\n",
    "        \n",
    "        # Transform phase\n",
    "        logger.info(\"-- TRANSFORM PHASE --\")\n",
    "        fact_data, customer_dim, time_dim = transform_data(df)\n",
    "        \n",
    "        # Load phase\n",
    "        logger.info(\"-- LOAD PHASE --\")\n",
    "        if not load_to_database(fact_data, customer_dim, time_dim):\n",
    "            raise RuntimeError(\"Database load failed\")\n",
    "        \n",
    "        logger.info(\"=== ETL PIPELINE COMPLETED SUCCESSFULLY ===\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"ETL pipeline failed: {str(e)}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Clear previous log file\n",
    "    if os.path.exists('etl.log'):\n",
    "        os.remove('etl.log')\n",
    "    \n",
    "    # Execute pipeline\n",
    "    success = run_etl_pipeline()\n",
    "    \n",
    "    # Final verification\n",
    "    if success and os.path.exists('retail_dw.db'):\n",
    "        logger.info(f\"Database verification: File size {os.path.getsize('retail_dw.db')} bytes\")\n",
    "        try:\n",
    "            with sqlite3.connect('retail_dw.db') as conn:\n",
    "                tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "                logger.info(f\"Tables created:\\n{tables}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Verification failed: {str(e)}\")\n",
    "    \n",
    "    exit(0 if success else 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
